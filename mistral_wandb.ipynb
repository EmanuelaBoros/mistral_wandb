{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urSaPCZoqEmM"
      },
      "source": [
        "# Mistral and Weights & Biases\n",
        "\n",
        "In this notebooks you will learn how to trace your MistralAI Api calls using W&B Weave, how to evaluate the performance of your models and how to close the gap by leveraging the MistralAI finetuning capabilities.\n",
        "\n",
        "- Weights & Biases: https://wandb.ai/\n",
        "- Mistral finetuning docs: https://docs.mistral.ai/capabilities/finetuning/\n",
        "- Tracing with W&B Weave: https://wandb.me/weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install mistralai pandas weave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Mistral and Weave\n",
        "\n",
        "You will probably integrate MistralAI API calls in your codebase by creating a function like the one below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, asyncio, json\n",
        "import weave\n",
        "from mistralai.async_client import MistralAsyncClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "client = MistralAsyncClient(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
        "\n",
        "@weave.op()  # <---- add this and you are good to go\n",
        "async def call_mistral(model:str, messages:list, **kwargs) -> str:\n",
        "    \"Call the Mistral API\"\n",
        "    chat_response = await client.chat(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        **kwargs,\n",
        "    )\n",
        "    return chat_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only thing you need to do is add the @weave.op() decorator to the function you want to trace.\n",
        "\n",
        "Let's define a more interesting function that recommends cheese based on the region and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "async def cheese_recommender(region:str, model:str) -> str:\n",
        "    \"Recommend the best cheese in a given region\"\n",
        "     \n",
        "    messages = [ChatMessage(\n",
        "        role=\"user\", \n",
        "        content=f\"What is the best cheese in {region}?\")]\n",
        "\n",
        "    cheeses = await call_mistral(model=model, messages=messages)\n",
        "    return {\"region\": region, \"cheeses\": cheeses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run this function and see how weave traces it. We call weave.init() to tell weave the project where to store the traces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "weave.init(\"mistral_webinar\")\n",
        "out = await cheese_recommender(region=\"France\", model=\"open-mistral-7b\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can view the traces by clicking the link above 👆\n",
        "![](cheese_recomender.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27PpM60GqRvR"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "Weave also has Dataset support, so you can keep your data and the model outputs in the same place. You can convert alsmot any iterable into a dataset!\n",
        "\n",
        "Let's load some Q/A data from our support [wandbot](https://github.com/wandb/wandbot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json('qa.jsonl', orient='records', lines=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train=df.sample(frac=0.9,random_state=200)\n",
        "df_eval=df.drop(df_train.index)\n",
        "len(df_train), len(df_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train = weave.Dataset(name=\"ds_train\", rows=df_train)\n",
        "ds_eval = weave.Dataset(name=\"ds_eval\", rows=df_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's publish them to Weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weave.publish(ds_train)\n",
        "weave.publish(ds_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A neat trick to get better answers is instead of passing a very long initial message, passing a small conversation with some prefilled agent responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_messages(question: str, cls=ChatMessage):\n",
        "    messages = [\n",
        "        cls(\n",
        "            role=\"user\", \n",
        "            content=(\n",
        "                \"You are an expert about Weights & Biases the ML platform. \"\n",
        "                 \"You will answer questions about the product, Answer the question directly, without repeating the instructions.\"\n",
        "                 )\n",
        "        ),\n",
        "        cls(\n",
        "            role=\"assistant\", \n",
        "            content=(\n",
        "                \"Sure, I'd be happy to help with your question about Weights & Biases. \"\n",
        "                 \"If you have a specific question about using Weights & Biases, such as how to track experiments, \"\n",
        "                 \"visualize data, or manage artifacts, please feel free to ask!\")\n",
        "        ),\n",
        "        cls(\n",
        "            role=\"user\", \n",
        "            content=f\"Here is the question: {question}\"\n",
        "        )\n",
        "    ]\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "async def wandb_expert(question:str, model:str) -> str:\n",
        "    \"Answer questions about wandb\"\n",
        "     \n",
        "    messages = create_messages(question=question)\n",
        "\n",
        "    answer = await call_mistral(model=model, messages=messages)\n",
        "    return {\"question\": question, \"answer\": answer}\n",
        "\n",
        "res = await wandb_expert(question=df.loc[0].question, model=\"mistral-medium-latest\")\n",
        "print(df.loc[0].question)\n",
        "print(res[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GT dataset\n",
        "Let's create a dataset with mistral-medium-latest as our baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MistralModel(weave.Model):\n",
        "    model: str\n",
        "    temperature: float = 0.7\n",
        "    \n",
        "    @weave.op\n",
        "    def create_messages(self, question:str):\n",
        "        return create_messages(question)\n",
        "\n",
        "    @weave.op\n",
        "    async def predict(self, question:str):\n",
        "        messages = self.create_messages(question)\n",
        "        return await call_mistral(model=self.model, messages=messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets create a dataset with the medium model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mistral_medium = MistralModel(model=\"mistral-medium-latest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def async_foreach(sequence, func, max_concurrent_tasks):\n",
        "    \"Handy parallelism async for looper\"\n",
        "    semaphore = asyncio.Semaphore(max_concurrent_tasks)\n",
        "    async def process_item(item):\n",
        "        async with semaphore:\n",
        "            result = await func(item)\n",
        "            return item, result\n",
        "\n",
        "    tasks = [asyncio.create_task(process_item(item)) for item in sequence]\n",
        "\n",
        "    for task in asyncio.as_completed(tasks):\n",
        "        item, result = await task\n",
        "        yield item, result\n",
        "        \n",
        "async def map(ds, func, max_concurrent_tasks = 7, col_name=\"model_preds\"):\n",
        "    new_dataset = []\n",
        "    async for example, map_results in async_foreach(ds.rows, func, max_concurrent_tasks):\n",
        "        example.update({col_name: map_results})\n",
        "        new_dataset.append(example)\n",
        "    return new_dataset\n",
        "\n",
        "ds_eval_medium_rows = await map(ds_eval, mistral_medium.predict, col_name=\"mistral_medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_eval_medium = weave.Dataset(name=\"ds_eval_medium\", description=\"Mistral medium predictions\", rows=ds_eval_medium_rows)\n",
        "weave.publish(ds_eval_medium)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can pull your data back easily using the API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_eval_medium = weave.ref('ds_eval_medium:latest').get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add the results of Mistral 7B (non finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mistral_7b = MistralModel(model=\"open-mistral-7b\")\n",
        "ds_eval_7b_rows = await map(ds_eval_medium, mistral_7b.predict, col_name=\"mistral_7b\")\n",
        "ds_eval_7b_medium = weave.Dataset(name=\"ds_eval_medium_7b\", description=\"Mistral 7b predictions along with medium\", rows=ds_eval_7b_rows)\n",
        "weave.publish(ds_eval_7b_medium)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](medium_7b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "Let's use mistral large as a judge, let's compute a score as baseline comparing `7B` and `medium`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLMJudge(weave.Model):\n",
        "    model: str = \"mistral-large-latest\"\n",
        "    \n",
        "    @weave.op\n",
        "    async def predict(self, question: str, mistral_7b: str, mistral_medium: str, answer: str, **kwargs) -> dict:\n",
        "        messages = [\n",
        "            ChatMessage(\n",
        "                role=\"user\",\n",
        "                content=(\n",
        "                \"You are an expert about Weights & Biases the ML platform. \"\n",
        "                \"You have to pick the best answer between two answers. \"\n",
        "                \"Take into consideration the context of the question and the ground truth answer as a reference. \\n\"\n",
        "                \"Here is the question: {question}\\n\"\n",
        "                \"Here is the answer1: {mistral_7b}\\n\"\n",
        "                \"Here is the answer2: {mistral_medium}\\n\"\n",
        "                \"Ground truth answer: {answer}\\n\"\n",
        "                \"Return the name of the best_answer (or None if you think both are wrong) and the reason in short JSON object.\").format(\n",
        "                    question=question, \n",
        "                    mistral_7b=mistral_7b, \n",
        "                    mistral_medium=mistral_medium,\n",
        "                    answer=answer)\n",
        "            )\n",
        "        ]\n",
        "        payload = await call_mistral(model=self.model, messages=messages, response_format={\"type\": \"json_object\"})\n",
        "        return json.loads(payload)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_eval_7b_medium.rows[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_judge = LLMJudge()\n",
        "res = await llm_judge.predict(**ds_eval_7b_medium.rows[0])\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def evaluate_answer(model_output: str) -> dict:\n",
        "    \"Evaluate the answer\"\n",
        "    return {\"win\": model_output[\"best_answer\"] == \"answer1\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a weave.evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation = weave.Evaluation(dataset=ds_eval_7b_medium, scorers=[evaluate_answer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await evaluation.evaluate(llm_judge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](eval_base.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tune FTW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is pretty descent for both 😍. Let's see if fine-tuning improves this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_messages(row):\n",
        "    \"Format on the expected MistralAI fine-tuning dataset\"\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    messages = create_messages(question, cls=dict)\n",
        "    # we need to append the answer for training 👇\n",
        "    messages = {\"messages\":messages + [dict(role=\"assistant\", content=answer)]}\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "msgs = format_messages(df_train.iloc[0])\n",
        "msgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUBRITvX59kC"
      },
      "outputs": [],
      "source": [
        "formatted_df_train = df_train.apply(format_messages, axis=1)\n",
        "formatted_df_eval = df_eval.apply(format_messages, axis=1)\n",
        "formatted_df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "formatted_df_train.to_json(\"formatted_df_train.jsonl\", orient=\"records\", lines=True)\n",
        "formatted_df_eval.to_json(\"formatted_df_eval.jsonl\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu_oLukAJect"
      },
      "source": [
        "## Upload dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0UlO1Qa7xi3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from mistralai.client import MistralClient\n",
        "\n",
        "api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
        "client = MistralClient(api_key=api_key)\n",
        "\n",
        "with open(\"formatted_df_train.jsonl\", \"rb\") as f:\n",
        "    ds_train = client.files.create(file=(\"formatted_df_train.jsonl\", f))\n",
        "with open(\"formatted_df_eval.jsonl\", \"rb\") as f:\n",
        "    ds_eval = client.files.create(file=(\"eval.jsonl\", f))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChnYoKhoapES"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def pprint(obj):\n",
        "    print(json.dumps(obj.dict(), indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIAJdJIc9g2q",
        "outputId": "f5fc042e-8c06-473b-a616-536a0c6dd30c"
      },
      "outputs": [],
      "source": [
        "pprint(ds_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uA-Xnp1RTmx",
        "outputId": "ac0bdaba-4af1-4f19-a8ab-dd19482b1943"
      },
      "outputs": [],
      "source": [
        "pprint(ds_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqchXMeXJi6U"
      },
      "source": [
        "## Create a fine-tuning job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Qk42ADRVKo"
      },
      "outputs": [],
      "source": [
        "from mistralai.models.jobs import TrainingParameters, WandbIntegrationIn\n",
        "\n",
        "created_jobs = client.jobs.create(\n",
        "    model=\"open-mistral-7b\",\n",
        "    training_files=[ds_train.id],\n",
        "    validation_files=[ds_eval.id],\n",
        "    hyperparameters=TrainingParameters(\n",
        "        training_steps=25,\n",
        "        learning_rate=0.0001,\n",
        "        ),\n",
        "    integrations=[\n",
        "        WandbIntegrationIn(\n",
        "            project=\"mistral_webinar\",\n",
        "            run_name=\"finetune_wandb\",\n",
        "            api_key=os.environ.get(\"WANDB_API_KEY\"),\n",
        "        ).dict()\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkZlYvwGanL1",
        "outputId": "396cd040-643b-4296-b026-bb3589df44de"
      },
      "outputs": [],
      "source": [
        "pprint(created_jobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQXIE2G3c-Ds",
        "outputId": "83b955b3-5666-4b23-abbe-fa4dbe0ec676"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "retrieved_job = client.jobs.retrieve(created_jobs.id)\n",
        "while retrieved_job.status in [\"RUNNING\", \"QUEUED\"]:\n",
        "    retrieved_job = client.jobs.retrieve(created_jobs.id)\n",
        "    pprint(retrieved_job)\n",
        "    print(f\"Job is {retrieved_job.status}, waiting 10 seconds\")\n",
        "    time.sleep(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can follow the training progress using in the wandb dashboard\n",
        "\n",
        "![](ft.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x6wRaDtXDzt",
        "outputId": "1efa4948-0ca8-4bc1-85d2-7b72b7e4a931"
      },
      "outputs": [],
      "source": [
        "# List jobs\n",
        "jobs = client.jobs.list()\n",
        "pprint(jobs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's retrieve the fie-tuned model. NOw we don't need to do any aditional setup, we can just use the model served for us using the MistralAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhTWA5uJXHNp",
        "outputId": "a6db9934-a231-4650-d539-17c25e32b8d5"
      },
      "outputs": [],
      "source": [
        "# Retrieve a jobs\n",
        "retrieved_jobs = client.jobs.retrieve(created_jobs.id)\n",
        "pprint(retrieved_jobs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK-cSS2EJv-e"
      },
      "source": [
        "## Use a fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compute the predictions using the fine-tuned 7B model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_eval_medium = weave.ref('ds_eval_medium:latest').get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add the results of Mistral 7B-finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mistral_7b_ft = MistralModel(model=retrieved_jobs.fine_tuned_model)\n",
        "ds_eval_7b_rows = await map(ds_eval_medium, mistral_7b_ft.predict, col_name=\"mistral_7b\")\n",
        "ds_eval_7b_ft_medium = weave.Dataset(name=\"ds_eval_medium_7b_ft\", description=\"Finetuned Mistral 7b predictions along with medium\", rows=ds_eval_7b_rows)\n",
        "weave.publish(ds_eval_7b_ft_medium)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation = weave.Evaluation(dataset=ds_eval_7b_ft_medium, scorers=[evaluate_answer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await evaluation.evaluate(llm_judge)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
