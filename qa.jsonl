{"question":"What is the difference between team and organization?","answer":"A team is a collaborative workspace for a group of users working on the same projects, while an organization is a higher-level entity that may consist of multiple teams and is often related to billing and account management."}
{"question":"What is the difference between team and entity? As a user - what does entity mean for me?","answer":"A team is a collaborative workspace for a group of users working on the same projects, while an entity refers to either a username or a team name. When you log runs in W&B, you can set the entity to your personal account or a team account `wandb.init(entity=\"example-team\")`."}
{"question":"What is a team and where can I find more information about it?","answer":"Use W&B Teams as a central workspace for your ML team to build better models faster.\n\n* **Track all the experiments** your team has tried so you never duplicate work.\n* **Save and reproduce** previously trained models.\n* **Share progress** and results with your boss and collaborators.\n* **Catch regressions** and immediately get alerted when performance drops.\n* **Benchmark model performance** and compare model versions.\n\nVisit the [team section](https:\/\/docs.wandb.ai\/guides\/app\/features\/teams) in the documentation to know more about teams. \n"}
{"question":"When should I log to my personal entity against my team entity?","answer":"You should log to your personal entity when you are working on individual projects or experiments that are not related to your team's work. On the other hand, you should log to your team entity when you are collaborating with your team members on a shared project or when the results of your experiments are relevant to the team's goals."}
{"question":"Who can create a team? Who can add or delete people from a team? Who can delete projects?","answer":"**Admin**: Team admins can add and remove other admins or team members. They have permissions to modify all projects and full deletion permissions. This includes, but is not limited to, deleting runs, projects, artifacts, and sweeps."}
{"question":"What type of roles are available and what are the differences between them?","answer":"There are following team role options:\n\n- **Admin**: Team admins can add and remove other admins or team members. They have permissions to modify all projects and full deletion permissions. This includes, but is not limited to, deleting runs, projects, artifacts, and sweeps.\n- **Member**: A regular member of the team. An admin invites a team member by email. A team member cannot invite other members. Team members can only delete runs and sweep runs created by that member. Suppose you have two members A and B. Member B moves a Run from team B's project to a different project owned by Member A. Member A can not delete the Run Member B moved to Member A's project. Only the member that creates the Run, or the team admin, can delete the run.\n- **Service (Enterprise-only feature)**: A service worker, an API key useful for using W&B with your run automation tools. If you use the API key from a service account for your team, make sure to set the environment variable **WANDB_USERNAME** to attribute runs to the correct user. See more on the relevant behavior below.\n- **View-Only (Enterprise-only feature)**: View-Only members can view assets within the team such as runs, reports, and workspaces. They can follow and comment on reports, but they can not create, edit, or delete project overview, reports, or runs. View-Only members do not have an API key.\n- **Custom roles (Enterprise-only feature)**: Custom roles allow organization admins to compose new roles by inheriting from the above **View-Only** or **Member** roles, and adding additional permissions to achieve fine-grained access control. Team admins can then assign any of those custom roles to users in their respective teams. Refer to [this article](https:\/\/wandb.ai\/wandb_fc\/announcements\/reports\/Introducing-Custom-Roles-for-W-B-Teams--Vmlldzo2MTMxMjQ3) for details. \n\n:::note\nW&B recommends to have more than one admin in a team. It is a best practice to ensure that admin operations can continue when the primary admin is not available.\n:::\n\n:::note\nIf you're on W&B Server (Dedicated Cloud or Self-managed deployment), you will need a updated enterprise license to use the **Custom Roles** feature.\n:::\n"}
{"question":"What are service accounts, and how do we add one to our team?","answer":"A service account (Enterprise-only feature) is an API key that has permissions to write to your team, but is not associated with a particular user. Among other things, service accounts are useful for tracking automated jobs logged to wandb, like periodic retraining, nightly builds, and so on. If you'd like, you can associate a username with one of these machine-launched runs with the environment variable WANDB_USERNAME.\n\n**Team Service Account Behavior**\n\n- When you configure a team in your training environment, you can use a service account from that team to log runs in either of private or public projects within that team. Additionally, you can attribute those runs to a user if **WANDB_USERNAME** or **WANDB_USER_EMAIL** variable exists in your environment and the referenced user is part of that team.\n- When you **do not** configure a team in your training environment and use a service account, the runs log to the named project within that service account's parent team. In this case as well, you can attribute the runs to a user if **WANDB_USERNAME** or **WANDB_USER_EMAIL** variable exists in your environment and the referenced user is part of the service account's parent team.\n- A service account can not log runs to a private project in a team different from its parent team, but it can log runs to public projects in other teams.\n\nYou can get the API key in your Team Settings page \/teams\/<your-team-name> where you invite new team members. Select service and click create to add a service account."}
{"question":"How can I see the bytes stored, bytes tracked and tracked hours of my organization?","answer":"\n- You can check the bytes stored of your organization at `https:\/\/<host-url>\/usage\/<team-name>`.\n- You can check the bytes tracked of your organization at `https:\/\/<host-url>\/usage\/<team-name>\/tracked`.\n- You can check the tracked hours of your organization at `https:\/\/<host-url>\/usage\/<team-name>\/computehour`.\n"}
{"question":"What really good functionalities are hidden and where can I find those?","answer":"We have some functionalities hidden under a feature flag in the \u201cBeta Features\u201d section. These can be enabled under the user settings page.\n\nHere are the available beta features hidden under the feature flag:\n\n- **Night mode**\n  - Description: Invert the colors everywhere! This makes pages dark, but might make colors on charts less easy to distinguish.\n\n- **Unicorn mode**\n  - Description: Change your cursor on charts from a boring pointer to a unicorn!\n\n- **Keyboard copy plot legend data**\n  - Description: When hovering over a plot in workspaces or a report, you can use Cmd+C to copy the run names and plot values shown in the hover control.\n"}
{"question":"Which files should I check when my code crashes?","answer":"For the affected run, you should check `debug.log` and `debug-internal.log`. These files are under your local folder `wandb\/run-<date>_<time>-<run-id>\/logs` in the same directory where you\u2019re running your code."}
{"question":"On a local instance, which files should I check when I have issues?","answer":"You should check the `Debug Bundle`. An admin of the instance can get it from the `\/system-admin` page -> top right corner W&B icon -> `Debug Bundle`.\n\nAn admin of the instance can access this by navigating to:\n\n1. The **System settings** found under the **W&B admin tools** section in the dropdown menu accessed by clicking on the user icon at the top right corner of the main page (as shown in the first image).\n2. Alternatively, the **Debug Bundle** can be accessed by clicking the **W&B icon** in the top right corner and selecting **Debug Bundle** from the dropdown menu (as shown in the second image).\n\nThis bundle contains logs and other data helpful for diagnosing issues with the system.\n"}
{"question":"If I am the admin of my local instance, how should I manage it?","answer":"The first user to sign up after the W&B Server instance is initially deployed is automatically assigned the instance `admin` role. The admin can then add additional users to the organization and create teams.\n\n:::note\nW&B recommends having more than one instance admin in an organization. It is a best practice to ensure that admin operations can continue when the primary admin is not available.\n:::\n\n**Manage your organization**\n\nAs an instance admin, you can invite, remove, and change a user's role. To do so, navigate to the Organization dashboard by selecting your profile image in the upper right-hand corner, and then click on **Organization dashboard**. \n\n**Invite users**\n\n1. Navigate to the W&B Organization dashboard.\n2. Click the **Add user** button as illustrated in the second image where the user is about to click the **Add user** button.\n3. Add the user's email in the Email field.\n4. Select the role you want to assign to the user, from `Admin, Member, or Viewer`. By default, all users are assigned a `Member` role.\n    - **Admin**: An instance admin who can add or remove other users to the organization, change user roles, manage custom roles, add teams, and more. W&B recommends more than one admin for an enterprise W&B server instance.\n    - **Member**: A regular user of the organization, invited by an instance admin. An organization user cannot invite other users or manage existing users in the organization. `Team admins` could add specific organization users to their respective teams (team-level roles described below in **Team roles**).\n    - **Viewer**: A view-only user of your organization, invited by an instance admin. A viewer only has read access to the organization and the underlying teams that they are a part of.\n5. Click the **Add new user** button.\n\nAn invite link will be sent to the user by email. Once the user accepts the invite, they will have access to the W&B instance (organization).\n\n**Remove a user**\n\n1. Navigate to the W&B Organization dashboard.\n2. Search for the user you want to modify in the search bar.\n3. Click on the meatball menu (three horizontal dots).\n4. Select **Remove user** as shown in the third image where the admin is interacting with the menu options for a user named \"<user-name>\".\n\n**Change a user's organization-level role**\n\n1. Navigate to the W&B Organization dashboard.\n2. Search for the user you want to modify in the search bar.\n3. Hover your mouse over the **Role** column. Click on the pencil icon that appears.\n4. From the dropdown, select a different role you want to assign.\n"}
{"question":"What does `wandb.init` do to my training process?","answer":"When `wandb.init()` is called from your training script an API call is made to create a run object on our servers. A new process is started to stream and collect metrics, thereby keeping all threads and logic out of your primary process. Your script runs normally and writes to local files, while the separate process streams them to our servers along with system metrics. You can always turn off streaming by running `wandb off` from your training directory, or setting the `WANDB_MODE` environment variable to `offline`."}
{"question":"Does your tool track or store training data?","answer":"W&B does not store any data unless `wandb.save` is called with the local file name. You can pass a SHA or other unique identifier to `wandb.config.update(...)` to associate a dataset with a training run."}
{"question":"What formula do you use for your smoothing algorithm?","answer":"We use the same exponential moving average formula as TensorBoard. \n\nAssuming all the real scalar values are in a list called scalars the smoothing is applied as follows:\n\n```\ndef smooth(scalars: List[float], weight: float) -> List[float]:  # Weight between 0 and 1\n    last = scalars[0]  # First value in the plot (first timestep)\n    smoothed = list()\n    for point in scalars:\n        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n        smoothed.append(smoothed_val)                        # Save it\n        last = smoothed_val                                  # Anchor the last smoothed value\n        \n    return smoothed\n```\n"}
{"question":"How do I get the random run name in my script?","answer":"Call `wandb.run.save()` and then get the name with `wandb.run.name`."}
{"question":"What is the difference between `.log()` and `.summary`?","answer":"The summary is the value that shows in the table while the log will save all the values for plotting later.\n\nFor example, you might want to call `wandb.log` every time the accuracy changes. Usually, you can just use .log. `wandb.log()` will also update the summary value by default unless you have set the summary manually for that metric\n\nThe scatterplot and parallel coordinate plots will also use the summary value while the line plot plots all of the values set by .log\n\nThe reason we have both is that some people like to set the summary manually because they want the summary to reflect for example the optimal accuracy instead of the last accuracy logged.\n"}
{"question":"How is W&B different from TensorBoard?","answer":"We love the TensorBoard folks, and we have a TensorBoard integration! We were inspired to improve experiment tracking tools for everyone. When the co-founders started working on W&B, they were inspired to build a tool for the frustrated TensorBoard users at OpenAI. Here are a few things we focused on improving:\n\n1. **Reproduce models**: W&B is good for experimentation, exploration, and reproducing models later. We capture not just the metrics, but also the hyperparameters and version of the code, and we can save your model checkpoints for you so your project is reproducible.\n2. **Automatic organization**: If you hand off a project to a collaborator or take a vacation, W&B makes it easy to see all the models you've tried so you're not wasting hours re-running old experiments.\n3. **Fast, flexible integration**: Add W&B to your project in 5 minutes. Install our free open-source Python package and add a couple of lines to your code, and every time you run your model you'll have nice logged metrics and records.\n4. **Persistent, centralized dashboard**: Anywhere you train your models, whether on your local machine, your lab cluster, or spot instances in the cloud, we give you the same centralized dashboard. You don't need to spend your time copying and organizing TensorBoard files from different machines.\n5. **Powerful table**: Search, filter, sort, and group results from different models. It's easy to look over thousands of model versions and find the best-performing models for different tasks. TensorBoard isn't built to work well on large projects.\n6. **Tools for collaboration**: Use W&B to organize complex machine learning projects. It's easy to share a link to W&B, and you can use private teams to have everyone send results to a shared project. We also support collaboration via reports\u2014 add interactive visualizations and describe your work in markdown. This is a great way to keep a work log, share findings with your supervisor, or present findings to your lab.\"\n\nWe also have a TensorBoard Integration, so you can use TensorBoard and W&B together. We have a guide on how to use them together [here](https:\/\/docs.wandb.ai\/guides\/integrations\/tensorboard)."}
{"question":"How does wandb stream logs and writes to disk?","answer":"W&B queues in memory but also write the events to disk asynchronously to handle failures and for the `WANDB_MODE=offline` case where you can sync the data after it's been logged.\n\nThe `DataStore` class handles the streaming of logs and writing to disk in a structured manner, particularly suitable for managing failure scenarios and supporting offline functionality.\n\n**Overview of `DataStore` Class**\n\nThe `DataStore` class uses a logging system based on the LevelDB log format to ensure efficient and secure data handling. It manages a datastore that allows writing records either fully or in segments (i.e., first, middle, last) depending on the size of the data and the remaining space in the block. Here's how data handling and logging are structured:\n\n1. **Initialization**: During initialization, the class sets up internal state management for data writing and ensures that it is only used in internal processes.\n\n2. **Writing Data**:\n   - Data is written in blocks of 32,768 bytes, minus the header size.\n   - Depending on the space left in the current block and the size of the data, the data might be split across multiple records (FULL, FIRST, MIDDLE, LAST).\n   - Each record includes a header with a checksum for data integrity, and its type (FULL, FIRST, MIDDLE, LAST).\n\n3. **Error Handling and Integrity**:\n   - To manage integrity, checksums are computed for each type of record, ensuring data is written correctly.\n   - The class includes methods to ensure data is fully flushed to disk, crucial for preventing data loss in case of failure.\n\n4. **Flush Management**:\n   - Data written is periodically flushed to disk to ensure it is stored permanently.\n   - The flush method is explicitly called after writing data, and an operating system-level sync is performed to ensure data is securely written to disk hardware.\n\n5. **Closing and Cleanup**:\n   - Upon closing the datastore, all resources are cleaned up properly to avoid data leaks and ensure all data is written and closed correctly.\n\nIn your terminal, you can see a path to the local run directory. This directory will contain a `.wandb` file that is the datastore above. If you're also logging images, we write them to `media\/images` in that directory before uploading them to cloud storage.\n"}
{"question":"How to get multiple charts with different selected runs?","answer":"With wandb reports the procedure is as follows:\n\n- Have multiple panel grids.\n- Add filters to filter the run sets of each panel grid. This will help in selecting the runs that you want to portray in the respective panels.\n- Create the charts you want in the panel grids.\n"}
{"question":"How is access to the API controlled?","answer":"For simplicity, W&B uses API keys for authorization when accessing the API. You can find your API keys in your [settings](https:\/\/app.wandb.ai\/settings). Your API key should be stored securely and never checked into version control. In addition to personal API keys, you can add Service Account users to your team."}
{"question":"Does W&B support SSO for SaaS?","answer":"Yes, W&B supports setting up Single Sign-On (SSO) for the SaaS offering via Auth0. W&B support SSO integration with any OIDC compliant identity provider(ex: Okta, AzureAD etc.). If you have an OIDC provider, please follow the steps below:\n\n- Create a `Single Page Application (SPA)` on your Identity Provider.\n- Set `grant_type` to `implicit` flow.\n- Set the callback URI to `https:\/\/wandb.auth0.com\/login\/callback`.\n\n**What W&B needs?**\n\nOnce you have the above setup, contact your customer success manager(CSM) and let us know the `Client ID` and `Issuer URL` associated with the application.\n\nWe'll then set up an Auth0 connection with the above details and enable SSO.\n"}
{"question":"How can I rotate or revoke access?","answer":"Both personal and service account keys can be rotated or revoked. Simply create a new API Key or Service Account user and reconfigure your scripts to use the new key. Once all processes are reconfigured, you can remove the old API key from your profile or team."}
{"question":"How do I switch between accounts on the same machine?","answer":"If you have two W&B accounts working from the same machine, you'll need a nice way to switch between your different API keys. You can store both API keys in a file on your machine then add code like the following to your repos. This is to avoid checking your secret key into a source control system, which is potentially dangerous.\n\n```\nif os.path.exists(\"~\/keys.json\"):\n    os.environ[\"WANDB_API_KEY\"] = json.loads(\"~\/keys.json\")[\"work_account\"]\n```\n"}
{"question":"Is there a dark mode?","answer":"Yes. To enable dark mode:\n\n1. Navigate to your account settings at [https:\/\/wandb.ai\/settings](https:\/\/wandb.ai\/settings).\n2. Scroll to the **Beta Features** section.\n3. Toggle the **Night mode** option.\n"}
{"question":"Can I disable wandb when testing my code?","answer":"By using `wandb.init(mode=\"disabled\")` or by setting `WANDB_MODE=disabled` you will make wandb act like a NOOP which is perfect for testing your code.\n\n**Note**: Setting `wandb.init(mode=\u201cdisabled\u201d)` does not prevent `wandb` from saving artifacts to `WANDB_CACHE_DIR`\n"}
{"question":"How often are system metrics collected?","answer":"By default, metrics are collected every 2 seconds and averaged over a 15-second period. If you need higher resolution metrics, email us a [contact@wandb.com](mailto:contact@wandb.com)."}
{"question":"Can I just log metrics, no code or dataset examples?","answer":"**Dataset Examples**\n\nBy default, we don't log any of your dataset examples. You can explicitly turn this feature on to see example predictions in our web interface.\n\n**Code Logging**\n\nThere are two ways to turn off code logging:\n\n1. Set `WANDB_DISABLE_CODE` to `true` to turn off all code tracking. We won't pick up the git SHA or the diff patch.\n2. Set `WANDB_IGNORE_GLOBS` to `*.patch` to turn off syncing the diff patch to our servers. You'll still have it locally and be able to apply it with the `wandb restore` command.\n"}
{"question":"Can I log metrics on two different time scales? (For example, I want to log training accuracy per batch and validation accuracy per epoch.)","answer":"Yes, you can do this by logging your indices (e.g. `batch` and `epoch`) whenever you log your other metrics. So in one step you could call `wandb.log({'train_accuracy': 0.9, 'batch': 200})` and in another step call `wandb.log({'val_accuracy': 0.8, 'epoch': 4})`. Then, in the UI, you can set the appropriate value as the x-axis for each chart. If you want to set the default x-axis of a particular index you can do so using by using `Run.define_metric()`. In our above example we could do the following:\n\n```\nwandb.init()\n\nwandb.define_metric(\"batch\")\nwandb.define_metric(\"epoch\")\n\nwandb.define_metric(\"train_accuracy\", step_metric=\"batch\")\nwandb.define_metric(\"val_accuracy\", step_metric=\"epoch\")\n```\n"}
{"question":"How can I log a metric that doesn't change over time such as a final evaluation accuracy?","answer":"Using `wandb.log({'final_accuracy': 0.9}` will work fine for this. By default `wandb.log({'final_accuracy'})` will update `wandb.summary['final_accuracy']`, which is the value shown in the runs table."}
{"question":"How can I log additional metrics after a run completes?","answer":"There are several ways to do this.\n\nFor complicated workflows, we recommend using multiple runs and setting group parameters in `wandb.init` to a unique value in all the processes that are run as part of a single experiment. The runs table will automatically group the table by the group ID and the visualizations will behave as expected. This will allow you to run multiple experiments and training runs as separate processes log all the results into a single place.\n\nFor simpler workflows, you can call `wandb.init` with `resume=True` and `id=UNIQUE_ID` and then later call `wandb.init` with the same `id=UNIQUE_ID`. Then you can log normally with `wandb.log` or `wandb.summary` and the runs values will update.\n"}
{"question":"Will wandb slow down my training?","answer":"W&B should have a negligible effect on your training performance if you use it normally. Normal use of wandb means logging less than once a second and logging less than a few megabytes of data at each step. W&B runs in a separate process and the function calls don't block, so if the network goes down briefly or there are intermittent read write issues on disk it should not affect your performance. It is possible to log a huge amount of data quickly, and if you do that you might create disk I\/O issues. If you have any questions, please don't hesitate to contact us."}
{"question":"How many runs to create per project?","answer":"We recommend you have roughly 10k runs per project max for performance reasons."}
{"question":"Best practices to organize hyperparameter searches","answer":"If 10k runs per project (approx.) is a reasonable limit then our recommendation would be to set tags in `wandb.init()` and have a unique tag for each search. This means that you'll easily be able to filter the project down to a given search by clicking that tag in the Project Page in the Runs Table. For example `wandb.init(tags=['your_tag'])`"}
{"question":"How do I ignore files?","answer":"You can edit the `wandb\/settings` file and set `ignore_globs` equal to a comma separated list of globs. You can also the `WANDB_IGNORE_GLOBS`. A common use case is to prevent the git patch that we automatically create from being uploaded i.e. `WANDB_IGNORE_GLOBS=*.patch`"}
{"question":"How do I get the name of a run?","answer":"If you'd like to use the run name from within your script, you can use `wandb.run.name` and you'll get the run name \u2014 \"blissful-waterfall-2\" for example.\nYou need to call save on the run before you can access the display name:\n\n```\nrun = wandb.init(...)\nrun.save()\nprint(run.name)\n```\n"}
{"question":"How can I push all saved files from local?","answer":"Call `wandb.save(\"*.pattern\")` once at the top of your script after `wandb.init`, then all files that match that pattern will save immediately once they're written to `wandb.run.dir`."}
{"question":"Can I remove local files that have already been synced to cloud storage?","answer":"There\u2019s a command `wandb sync --clean` that you can run to remove local files that have already been synced to cloud storage. More information about usage can be found with `wandb sync --help`"}
{"question":"What if I want to restore the state of my code?","answer":"Use the `restore` command of our command line tool to return to the state of your code when you ran a given run.\n\n```\n# creates a branch and restores the code to the state\n# it was in when run $RUN_ID was executed\nwandb restore $RUN_ID\n```\n"}
{"question":"How does `wandb` capture the state of the code?","answer":"When `wandb.init` is called from your script, a link is saved to the last git commit if the code is in a git repository. A diff patch is also created in case there are uncommitted changes or changes that are out of sync with your remote."}
{"question":"How can I configure the name of the run in my training code?","answer":"At the top of your training script when you call `wandb.init`, pass in an experiment name, like this: `wandb.init(name=\"my_awesome_run\")`."}
{"question":"Can I run wandb offline?","answer":"If you're training on an offline machine and want to upload your results to our servers afterwards, we have a feature for you!\n\n1. Set the environment variable `WANDB_MODE=offline` to save the metrics locally, no internet required.\n2. When you're ready, run `wandb init` in your directory to set the project name.\n3. Run `wandb sync YOUR_RUN_DIRECTORY` to push the metrics to our cloud service and see your results in our hosted web app.\n\nYou can check via API whether your run is offline by using `run.settings._offline` or `run.settings.mode` after your wandb.init().\n"}
{"question":"What are some use-cases where you can use wandb sync","answer":"\n\n- If you don\u2019t have internet.\n- If you need to fully disable things.\n- To sync your run later due to any reason. For instance: if you want to avoid using resources on a training machine.\n"}
{"question":"Is there an anaconda package?","answer":"Yes! You can either install with `pip` or with `conda`. For the latter, you'll need to get the package from the conda-forge channel.\n\n```\n# Create a conda env\nconda create -n wandb-env python=3.8 anaconda\n# Activate created env\nconda activate wandb-env\n# install wandb with pip in this conda env\npip install wandb\n```\n\nAlternatively, \n\n```\nconda activate myenv\nconda install wandb --channel conda-forge\n```\n\nIf you run into issues with this install, please let us know. The Anaconda doc on managing packages - user-guide\/tasks\/manage-pkgs.html has some helpful guidance.\"\n"}
{"question":"How do I install the wandb Python library in environments without gcc?","answer":"If you try to install `wandb` and see this error:\n\n```\nunable to execute 'gcc': No such file or directory\nerror: command 'gcc' failed with exit status 1\n```\n\nYou can install `psutil` directly from a pre-built wheel. Find your Python version and OS here:\n\n```bash\nWHEEL_URL=<url to psutil-5.7.0-cp38-cp38-manylinux2010_x86_64.whl>\npip install $WHEEL_URL\n```\n\nAfter `psutil` has been installed, you can install wandb with `pip install wandb`.\n"}
{"question":"Does the W&B client support Python 2?","answer":"The W&B client library supported both Python 2.7 and Python 3 through version 0.10. Due to the Python 2 end of life, support for Python 2.7 was discontinued as of version 0.11. Users who run`pip install --upgrade wandb` on a Python 2.7 system will get new releases of the 0.10.x series only. Support for the 0.10.x series will be limited to critical bugfixes and patches. Currently, version 0.10.33 is the last version of the 0.10.x series that supports Python 2.7."}
{"question":"Does the W&B client support Python 3.5?","answer":"The W&B client library supported both Python 3.5 through version 0.11. Due to the Python 3.5 end of life, support was discontinued as of version 0.12"}
{"question":"If wandb crashes, will it possibly crash my training run?","answer":"It is extremely important to us that we never interfere with your training runs. We run wandb in a separate process to make sure that if wandb somehow crashes, your training will continue to run. If the internet goes out, wandb will continue to retry sending data to [wandb.ai](https:\/\/wandb.ai)."}
{"question":"Why is a run marked crashed in W&B when it\u2019s training fine locally?","answer":"This is likely a connection problem \u2014 if your server loses internet access and data stops syncing to W&B, we mark the run as crashed after a short period of retrying."}
{"question":"Does logging block my training? Is the logging function lazy? I don't want to be dependent on the network to send the results to your servers and then carry on with my local operations.","answer":"Calling `wandb.log` writes a line to a local file; it does not block any network calls. When you call `wandb.init` we launch a new process on the same machine that listens for filesystem changes and talks to our web service asynchronously from your training process."}
{"question":"How do I stop wandb from writing to my terminal or my jupyter notebook output?","answer":"Set the environment variable `WANDB_SILENT` to `true`.\n\nFor example, in your python code\n\n```\nos.environ[\"WANDB_SILENT\"] = \"true\"\n```\n\nor in notebooks, \n```\n%env WANDB_SILENT=true\n```\nor in the shell\n\n```python\nWANDB_SILENT=true\n```\n"}
{"question":"How do I kill a script that has wandb?","answer":"Press `Ctrl+D` on your keyboard to stop a script that is instrumented with wandb."}
{"question":"How do I deal with network issues?","answer":"If you're seeing SSL or network errors:`wandb: Network error (ConnectionError), entering retry loop`. You can try a couple of different approaches to solving this issue:\n\n1. Upgrade your SSL certificate. If you're running the script on an Ubuntu server, run `update-ca-certificates` We can't sync training logs without a valid SSL certificate because it's a security vulnerability.\n2. If your network is flaky, run training in offline mode and sync the files to us from a machine that has Internet access.\n3. Try running W&B Private Hosting, which operates on your machine and doesn't sync files to our cloud servers.\n\n`SSL CERTIFICATE_VERIFY_FAILED`: this error could be due to your company's firewall. You can set up local CAs and then use: `export REQUESTS_CA_BUNDLE=\/etc\/ssl\/certs\/ca-certificates.crt`\n"}
{"question":"What happens if internet connection is lost while I'm training a model?","answer":"If the wandb library is unable to connect to the internet it will enter a retry loop and keep attempting to stream metrics until the network is restored. During this time your program is able to continue running.\n\nIf you need to run on a machine without internet, you can set `WANDB_MODE=offline` to only have metrics stored locally on your hard drive. Later you can call `wandb sync DIRECTORY` to have the data streamed to our server.\n"}
{"question":"How do I launch multiple runs from one script?","answer":"Use `wandb.init` and `run.finish()` to log multiple Runs from one script:\n\n1. `run = wandb.init(reinit=True)`: Use this setting to allow reinitializing runs\n2. `run.finish()`: Use this at the end of your run to finish logging for that run\n\n```python\nimport wandb\n\nfor x in range(10):\n    run = wandb.init(reinit=True)\n    for y in range(100):\n        wandb.log({\"metric\": x + y})\n    run.finish()\n```\n\nAlternatively, you can use a python context manager which will automatically finish logging:\n\n```python\nimport wandb\n\nfor x in range(10):\n    with wandb.init(reinit=True) as run:\n        for y in range(100):\n            run.log({\"metric\": x + y})\n```\n"}
{"question":"I'm getting the following error : `InitStartError: Error communicating with wandb process` ","answer":"This error indicates that the library is having difficulty launching the process which synchronizes data to the server.\n\nThe following workarounds can help resolve the issue in certain environments:\n\n```\nwandb.init(settings=wandb.Settings(start_method=\"fork\"))\n```\n\nFor versions prior to `0.13.0` we suggest using:\n\n```\nwandb.init(settings=wandb.Settings(start_method=\"thread\"))\n```\n"}
{"question":"How can I use wandb with multiprocessing, e.g. distributed training?","answer":"If your training program uses multiple processes you will need to structure your program to avoid making wandb method calls from processes where you did not run `wandb.init()`.There are two major approaches to managing multiprocess training:\n\n1. Call `wandb.init` in all your processes, using the group keyword argument to define a shared group. Each process will have its own wandb run and the UI will group the training processes together.\n2. Call `wandb.init` from just one process and pass data to be logged over multiprocessing queues.\n\n\nIn distributed training, models are trained using multiple GPUs in parallel. W&B supports two patterns to track distributed training experiments:\n\n1. **One process**: Initialize W&B `wandb.init` and log experiments `wandb.log` from a single process. This is a common solution for logging distributed training experiments with the PyTorch Distributed Data Parallel (DDP) Class. In some cases, users funnel data over from other processes using a multiprocessing queue (or another communication primitive) to the main logging process.\n2. **Many processes**: Initialize W&B `wandb.init` and log experiments `wandb.log` in every process. Each process is effectively a separate experiment. Use the `group` parameter when you initialize W&B (`wandb.init(group='group-name')`) to define a shared experiment and group the logged values together in the W&B App UI.\n\nThe following examples demonstrate how to track metrics with W&B using PyTorch DDP on two GPUs on a single machine. PyTorch DDP (`DistributedDataParallel` in`torch.nn`) is a popular library for distributed training. The basic principles apply to any distributed training setup, but the details of implementation may differ.\n\n**Method 1: One process**\n\nIn this method we track only a rank 0 process. To implement this method, initialize W&B (`wandb.init)`, commence a W&B Run, and log metrics (`wandb.log`) within the rank 0 process. This method is simple and robust, however, this method does not log model metrics from other processes (for example, loss values or inputs from their batches). System metrics, such as usage and memory, are still logged for all GPUs since that information is available to all processes.\n\n:::info\n**Use this method to only track metrics available from a single process**. Typical examples include GPU\/CPU utilization, behavior on a shared validation set, gradients and parameters, and loss values on representative data examples.\n:::\n\nWithin your script, check to see if the rank is 0. To do so, first launch multiple processes with `torch.distributed.launch`. Next, check the rank with the `--local_rank` command line argument. If the rank is set to 0, we set up `wandb` logging conditionally in the `train()` function. Within your Python script, use the following check:\n\n```\nif __name__ == \"__main__\":\n    # Get args\n    args = parse_args()\n\n    if args.local_rank == 0:  # only on main process\n        # Initialize wandb run\n        run = wandb.init(\n            entity=args.entity,\n            project=args.project,\n        )\n        # Train model with DDP\n        train(args, run)\n    else:\n        train(args)\n```\n\nExplore the W&B App UI to view the dashboard of metrics tracked from a single process. The dashboard displays system metrics such as temperature and utilization, that are tracked for all GPUs.\n\nHowever, the loss values and other metrics such as epoch and batch size are only logged from a single `rank==0` GPU.\n```\n\n**Method 2: Many processes**\n\nIn this method, we track each process in the job, calling `wandb.init()` and `wandb.log()` from each process separately. We suggest you call `wandb.finish()` at the end of training, to mark that the run has completed so that all processes exit properly.\n\nThis method makes more information accessible for logging. However, note that multiple W&B Runs are reported in the W&B App UI. It might be difficult to keep track of W&B Runs across multiple experiments. To mitigate this, provide a value to the group parameter when you initialize W&B to keep track of which W&B Run belongs to a given experiment.\n\n:::info\n**Use this method if you want to track metrics from individual processes**. Typical examples include the data and predictions on each node (for debugging data distribution) and metrics on individual batches outside of the main node. This method is not necessary to get system metrics from all nodes nor to get summary statistics available on the main node.\n:::\n\nThe following Python code snippet demonstrates how to set the group parameter when you initialize W&B:\n\n```\nif __name__ == \"__main__\":\n    # Get args\n    args = parse_args()\n    # Initialize run\n    run = wandb.init(\n        entity=args.entity,\n        project=args.project,\n        group=\"DDP\",  # all runs for the experiment in one group\n    )\n    # Train model with DDP\n    train(args, run)\n```\n\nExplore the W&B App UI to view an the dashboard of metrics tracked from multiple processes. Note that there will be multiple W&B Runs grouped together in the left sidebar. Click on a group to view the dedicated group page for the experiment. The dedicated group page displays metrics from each process separately.\n\nIf you expand the group (select the Group dropdown) you will see the W&B Runs that are associated to that experiment.\n"}
{"question":"How do I avoid common distributed training issues?","answer":"Use W&B Service to avoid common distributed training issues.\n\nThere are two common issues you might encounter when using W&B and distributed training:\n\n1. **Hanging at the beginning of training** - A `wandb` process can hang if the `wandb` multiprocessing interferes with the multiprocessing from distributed training.\n2. **Hanging at the end of training** - A training job might hang if the `wandb` process does not know when it needs to exit. Call the `wandb.finish()` API at the end of your Python script to tell W&B that the Run finished. The wandb.finish() API will finish uploading data and will cause W&B to exit.\n\nWe recommend using the `wandb service` to improve the reliability of your distributed jobs. Both of the preceding training issues are commonly found in versions of the W&B SDK where wandb service is unavailable.\"\n"}
{"question":"How do I enable W&B Service?","answer":"Depending on your version of the W&B SDK, you might already have W&B Service enabled by default.\n\n**W&B SDK 0.13.0 and above**\n\nW&B Service is enabled by default for versions of the W&B SDK `0.13.0` and above.\n\n**W&B SDK 0.12.5 and above**\n\nModify your Python script to enable W&B Service for W&B SDK version 0.12.5 and above. Use the `wandb.require` method and pass the string `\"service\"` within your main function:\n\n```\nif __name__ == \"__main__\":\n    main()\n\n\ndef main():\n    wandb.require(\"service\")\n    # rest-of-your-script-goes-here\n```\n\nFor optimal experience we do recommend you upgrade to the latest version.\n\n**W&B SDK 0.12.4 and below**\n\nSet the `WANDB_START_METHOD` environment variable to `\"thread\"` to use multithreading instead if you use a W&B SDK version 0.12.4 and below.\n\n"}
{"question":"Can you show me examples of use cases for multiprocessing?","answer":"The following code snippets demonstrate common methods for advanced distributed use cases.\n\n**Spawn process**\n\nUse the `wandb.setup()`method in your main function if you initiate a W&B Run in a spawned process:\n\n```\nimport multiprocessing as mp\n\n\ndef do_work(n):\n    run = wandb.init(config=dict(n=n))\n    run.log(dict(this=n * n))\n\n\ndef main():\n    wandb.setup()\n    pool = mp.Pool(processes=4)\n    pool.map(do_work, range(4))\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Share a W&B Run**\n\nPass a W&B Run object as an argument to share W&B Runs between processes:\n\n```\ndef do_work(run):\n    run.log(dict(this=1))\n\n\ndef main():\n    run = wandb.init()\n    p = mp.Process(target=do_work, kwargs=dict(run=run))\n    p.start()\n    p.join()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\n:::info\nNote that we can not guarantee the logging order. Synchronization should be done by the author of the script.\n:::\n"}
{"question":"Can I just set the run name to the run ID?","answer":"If you'd like to overwrite the run name (like snowy-owl-10) with the run ID (like qvlp96vk) you can use this snippet:\n\n```python\nimport wandb\n\nwandb.init()\nwandb.run.name = wandb.run.id\nwandb.run.save()\n```\n"}
{"question":"I didn't name my run. Where is the run name coming from?","answer":"If you do not explicitly name your run, a random run name will be assigned to the run to help identify the run in the UI. For instance, random run names will look like \"pleasant-flower-4\" or \"misunderstood-glade-2\"."}
{"question":"How can I save the git commit associated with my run?","answer":"When `wandb.init` is called in your script, we automatically look for git information to save, including a link to a remote repo and the SHA of the latest commit. The git information should show up on your run page. If you aren't seeing it appear there, make sure that your shell's current working directory when executing your script is located in a folder managed by git.\n\nThe git commit and command used to run the experiment are visible to you but are hidden to external users, so if you have a public project, these details will remain private.\n"}
{"question":"Is it possible to save metrics offline and sync them to W&B later?","answer":"By default, `wandb.init` starts a process that syncs metrics in real time to our cloud hosted app. If your machine is offline, you don't have internet access, or you just want to hold off on the upload, here's how to run `wandb` in offline mode and sync later.\n\nYou will need to set two environment variables.\n\n1. `WANDB_API_KEY=$KEY`, where `$KEY` is the API Key from your settings page\n2. `WANDB_MODE=\"offline\"`\n\nAnd here's a sample of what this would look like in your script:\n\n```\nimport wandb\nimport os\n\nos.environ[\"WANDB_API_KEY\"] = <YOUR_KEY_HERE>\nos.environ[\"WANDB_MODE\"] = \"offline\"\n\nconfig = {\n    \"dataset\": \"CIFAR10\",\n    \"machine\": \"offline cluster\",\n    \"model\": \"CNN\",\n    \"learning_rate\": 0.01,\n    \"batch_size\": 128,\n}\n\nwandb.init(project=\"offline-demo\")\n\nfor i in range(100):\n    wandb.log({\"accuracy\": i})\n```\n\nAnd once you're ready, just run a sync command to send that folder to the cloud.\n\n```\nwandb sync wandb\/dryrun-folder-name\n```\n"}
{"question":"What is the difference between wandb.init modes?","answer":"Modes can be \"online\", \"offline\" or \"disabled\", and default to online.\n\n- `online`(default): In this mode, the client sends data to the wandb server.\n- `offline`: In this mode, instead of sending data to the wandb server, the client will store data on your local machine which can be later synced with the `wandb sync` command.\n- `disabled`: In this mode, the client returns mocked objects and prevents all network communication. The client will essentially act like a no-op. In other words, all logging is entirely disabled. However, stubs out of all the API methods are still callable. This is usually used in tests."}
{"question":"My run's state is \"crashed\" on the UI but is still running on my machine. What do I do to get my data back?","answer":"You most likely lost connection to your machine while training. You can recover your data by running `wandb sync [PATH_TO_RUN]`. The path to your run will be a folder in your `wandb` directory corresponding to the Run ID of the run in progress."}
{"question":"`I'm seeing the following error: `LaunchError: Permission denied`, what do I do?","answer":"If you're getting the error message `Launch Error: Permission denied`, you don't have permissions to log to the project you're trying to send runs to. This might be for a few different reasons.\n\n1. You aren't logged in on this machine. Run `wandb login` on the command line.\n2. You've set an entity that doesn't exist. \"Entity\" should be your username or the name of an existing team. If you need to create a team, go to our Subscriptions page.\n3. You don't have project permissions. Ask the creator of the project to set the privacy to **Open** so you can log runs to this project.\n"}
{"question":"Does W&B uses the `multiprocessing` library?","answer":"Yes, W&B uses the `multiprocessing` library. If you see an error message such as:\n\n```\nAn attempt has been made to start a new process before the current process \nhas finished its bootstrapping phase.\n```\n\nThis might mean that you might need to add an entry point protection `if name == main`. Note that you would only need to add this entry point protection in case you're trying to run W&B directly from the script.\n"}
{"question":"How can I organize my logged charts and media in the W&B UI?","answer":"We treat `\/` as a separator for organizing logged panels in the W&B UI. By default, the component of the logged item's name before a `\/` is used to define a group of panel called a \"Panel Section\".\n\n```\nwandb.log({\"val\/loss\": 1.1, \"val\/acc\": 0.3})\nwandb.log({\"train\/loss\": 0.1, \"train\/acc\": 0.94})\n```\n\nIn the Workspace settings, you can change whether panels are grouped by just the first component or by all components separated by `\/`.\n"}
{"question":"How can I compare images or media across epochs or steps?","answer":"Each time you log images from a step, we save them to show in the UI. Expand the image panel, and use the step slider to look at images from different steps. This makes it easy to compare how a model's output changes during training."}
{"question":"What if I want to log some metrics on batches and some metrics only on epochs?","answer":"If you'd like to log certain metrics in every batch and standardize plots, you can log x axis values that you want to plot with your metrics. Then in the custom plots, click edit and select a custom x-axis.\n\n```\nwandb.log({\"batch\": batch_idx, \"loss\": 0.3})\nwandb.log({\"epoch\": epoch, \"val_acc\": 0.94})\n```\n"}
{"question":"How do I log a list of values?","answer":" You can log a list of values iteratively, or using `wandb.Histogram` depending on your usecase.\n\nTo log values iteratively, use\n\n```\nwandb.log({f\"losses\/loss-{ii}\": loss for ii, loss in enumerate(losses)})\n```\n\nAlternatively, you can use `wandb.Histogram` to log a list of values as a single histogram.\n\n```python\nwandb.log({\"losses\": wandb.Histogram(losses)})  # converts losses to a histogram\n```\n"}
{"question":"How do I plot multiple lines on a plot with a legend?","answer":"Multi-line custom chart can be created by using `wandb.plot.line_series()`. You'll need to navigate to the project page to see the line chart. To add a legend to the plot, pass the keys argument within `wandb.plot.line_series()`. For example:\n\n```\nwandb.log(\n    {\n        \"my_plot\": wandb.plot.line_series(\n            xs=x_data, ys=y_data, keys=[\"metric_A\", \"metric_B\"]\n        )\n    }\n)\n```\n"}
{"question":"How do I add Plotly\/Bokeh Charts into Tables?","answer":"Adding Plotly\/Bokeh figures to Tables directly is not yet supported. Instead, write the figure to HTML and then add the HTML to the Table. \nHere are two examples with interactive Plotly and Bokeh charts.\n\n```\nimport wandb\nimport plotly.express as px\n\n# Initialize a new run\nrun = wandb.init(project=\"log-plotly-fig-tables\", name=\"plotly_html\")\n\n# Create a table\ntable = wandb.Table(columns=[\"plotly_figure\"])\n\n# Create path for Plotly figure\npath_to_plotly_html = \".\/plotly_figure.html\"\n\n# Example Plotly figure\nfig = px.scatter(x=[0, 1, 2, 3, 4], y=[0, 1, 4, 9, 16])\n\n# Write Plotly figure to HTML\n# Setting auto_play to False prevents animated Plotly\n# charts from playing in the table automatically\nfig.write_html(path_to_plotly_html, auto_play=False)\n\n# Add Plotly figure as HTML file into Table\ntable.add_data(wandb.Html(path_to_plotly_html))\n\n# Log Table\nrun.log({\"test_table\": table})\nwandb.finish()\n```\n\nAlternatively, using `bokeh`\n\n```\nfrom scipy.signal import spectrogram\nimport holoviews as hv\nimport panel as pn\nfrom scipy.io import wavfile\nimport numpy as np\nfrom bokeh.resources import INLINE\n\nhv.extension(\"bokeh\", logo=False)\nimport wandb\n\n\ndef save_audio_with_bokeh_plot_to_html(audio_path, html_file_name):\n    sr, wav_data = wavfile.read(audio_path)\n    duration = len(wav_data) \/ sr\n    f, t, sxx = spectrogram(wav_data, sr)\n    spec_gram = hv.Image((t, f, np.log10(sxx)), [\"Time (s)\", \"Frequency (hz)\"]).opts(\n        width=500, height=150, labelled=[]\n    )\n    audio = pn.pane.Audio(wav_data, sample_rate=sr, name=\"Audio\", throttle=500)\n    slider = pn.widgets.FloatSlider(end=duration, visible=False)\n    line = hv.VLine(0).opts(color=\"white\")\n    slider.jslink(audio, value=\"time\", bidirectional=True)\n    slider.jslink(line, value=\"glyph.location\")\n    combined = pn.Row(audio, spec_gram * line, slider).save(html_file_name)\n\n\nhtml_file_name = \"audio_with_plot.html\"\naudio_path = \"hello.wav\"\nsave_audio_with_bokeh_plot_to_html(audio_path, html_file_name)\n\nwandb_html = wandb.Html(html_file_name)\nrun = wandb.init(project=\"audio_test\")\nmy_table = wandb.Table(columns=[\"audio_with_plot\"], data=[[wandb_html], [wandb_html]])\nrun.log({\"audio_table\": my_table})\nrun.finish()\n```\n"}
{"question":"Why is nothing showing up in my graphs?","answer":"If you're seeing \"No visualization data logged yet\" that means that we haven't gotten the first `wandb.log` call from your script yet. This could be because your run takes a long time to finish a step. If you're logging at the end of each epoch, you could log a few times per epoch to see data stream in more quickly."}
{"question":"Why is the same metric appearing more than once?","answer":"If you're logging different types of data under the same key, we have to split them out in our database. This means you'll see multiple entries of the same metric name in a dropdown in the UI. The types we group by are `number`, `string`, `bool`, `other` (mostly arrays), and any `wandb` data type (`Histogram`, `Image`, etc). Send only one type to each key to avoid this behavior.\n\nWe store metrics in a case-insensitive fashion, so make sure you don't have two metrics with the same name like `\"My-Metric\"` and `\"my-metric\"`.\n"}
{"question":"How can I access the data logged to my runs directly and programmatically?","answer":"The history object is used to track metrics logged by `wandb.log`. Using our import\/export API, you can access the history object via `run.history()`.\n\n```python\napi = wandb.Api()\nrun = api.run(\"username\/project\/run_id\")\nprint(run.history())\n```\n"}
{"question":"What happens when I log millions of steps to W&B? How is that rendered in the browser?","answer":"The more points you send us, the longer it will take to load your graphs in the UI. If you have more than 1000 points on a line, we sample down to 1000 points on the backend before we send your browser the data. This sampling is nondeterministic, so if you refresh the page you'll see a different set of sampled points.\n\n**Guidelines**\n\nWe recommend that you try to log less than 10,000 points per metric. If you log more than 1 million points in a line, it will take us while to load the page. For more on strategies for reducing logging footprint without sacrificing accuracy. If you have more than 500 columns of config and summary metrics, we'll only show 500 in the table.\n"}
{"question":"What if I want to integrate W&B into my project, but I don't want to upload any images or media?","answer":"W&B can be used even for projects that only log scalars \u2014 you specify any files or data you'd like to upload explicitly."}
{"question":"What happens if I pass a class attribute into wandb.log()?","answer":"It is generally not recommended to pass class attributes into `wandb.log()` as the attribute may change before the network call is made. If you are storing metrics as the attribute of a class, it is recommended to deep copy the attribute to ensure the metric logged matches the value of the attribute at the time that `wandb.log()` was called."}
{"question":"Why am I seeing fewer data points than I logged?","answer":"If you are visualizing your metrics against something other than `Step` on your X-Axis, you might see fewer data points than you expect. This is because we require the metrics to be plotted against one another to be logged at the same `Step` - that is how we keep your metrics synchronized, i.e., we only sample metrics that are logged at the same `Step` while interpolating in between samples.\n**Guidelines**\n\nWe recommend you bundle your metrics into the same `log()` call. If your code looks like this:\n\n```\nwandb.log({\"Precision\": precision})\n...\nwandb.log({\"Recall\": recall})\n```\n\nIt would be better to log it as:\n\n```\nwandb.log({\"Precision\": precision, \"Recall\": recall})\n```\n\nAlternatively, you can manually control the step parameter and synchronize your metrics in your own code:\n\n```\nwandb.log({\"Precision\": precision}, step=step)\n...\nwandb.log({\"Recall\": recall}, step=step)\n```\n\nIf the value of `step` is the same in both the calls to `log()`, your metrics will be logged under the same step and be sampled together. Please note that step must be monotonically increasing in each call, otherwise the `step` value is ignored during your call to `log()`.\n"}
{"question":"How can I compare images or media across epochs or steps?","answer":"Each time you log images from a step, we save them to show in the UI. Expand the image panel, and use the step slider to look at images from different steps. This makes it easy to compare how a model's output changes during training."}
{"question":"How do I log a PNG?","answer":"`wandb.Image` converts `numpy` arrays or instances of `PILImage` to PNGs by default.\n\n```\nwandb.log({\"example\": wandb.Image(...)})\n# Or multiple images\nwandb.log({\"example\": [wandb.Image(...) for img in images]})\n```\n\n"}
{"question":"How do I log a video?","answer":"Videos are logged using the `wandb.Video`data type:\n\n```\nwandb.log({\"example\": wandb.Video(\"myvideo.mp4\")})\n```\n\nNow you can view videos in the media browser. Go to your project workspace, run workspace, or report and click \"Add visualization\" to add a rich media panel.\n\n"}
{"question":"How do I navigate and zoom in point clouds?","answer":"You can hold control and use the mouse to move around inside the space."}
{"question":"How do I log a 2D view of a molecule?","answer":"You can log a 2D view of a molecule using the `wandb.Image` data type and `rdkit`:\n\n```\nmolecule = rdkit.Chem.MolFromSmiles(\"CC(=O)O\")\nrdkit.Chem.AllChem.Compute2DCoords(molecule)\nrdkit.Chem.AllChem.GenerateDepictionMatching2DStructure(molecule, molecule)\npil_image = rdkit.Chem.Draw.MolToImage(molecule, size=(300, 300))\n\nwandb.log({\"acetic_acid\": wandb.Image(pil_image)})\n```\n"}
{"question":"How do I set the WANDB_NOTEBOOK_NAME?","answer":"If you're seeing the error message \"Failed to query for notebook name, you can set it manually with the `WANDB_NOTEBOOK_NAME` environment variable,\" you can resolve it by setting the environment variable. There's multiple ways to do so:\n\nusing jupyter %magic\n\n```\n%env \"WANDB_NOTEBOOK_NAME\" \"notebook name here\"\n\n```\n\nin a code cell\n\n```\nimport os\n\nos.environ[\"WANDB_NOTEBOOK_NAME\"] = \"notebook name here\"\n```\n\n"}
{"question":"How do Rate Limits in wandb work?","answer":"The W&B API is rate limited by IP and API key. Free accounts are restricted to 50 requests per minute. Paid accounts are restricted to 200 requests per minute. This rate allows you to run approximately 15 processes in parallel and have them report without being throttled. If the wandb client detects it's being limited, it will backoff and retry sending the data in the future. If you need to run more than 15 processes in parallel send an email to contact@wandb.com.\n\nFor sweeps, we support up to 20 parallel agents."}
{"question":"I do not want W&B to build a container for me, can I still use Launch?","answer":"Yes. Run the following to launch a pre-built docker image. Replace the items in the `<>` with your information:\n\n```\nwandb launch -d <docker-image-uri> -q <queue-name> -E <entrypoint>\n```  \n\nThis will build a job when you create a run.\n\nOr you can make a job from an image:\n\n```\nwandb job create image <image-name> -p <project> -e <entity>\n```\n"}
{"question":"Are there best practices for using Launch effectively?","answer":"Yes. Here are some best practices to consider when using Launch:\n\n  1. Create your queue before you start your agent, so that you can set your agent to point to it easily.  If you don\u2019t do this, your agent will give errors and not work until you add a queue.\n  2. Create a W&B service account to start up the agent, so that it's not tied to an individual user account.\n  3. Use `wandb.config` to read and write your hyperparameters, so that they can be overwritten when re-running a job.\n"}
{"question":"I do not like clicking- can I use Launch without going through the UI?","answer":"Yes. The standard `wandb` CLI includes a `launch` subcommand that you can use to launch your jobs. For more info, try running\n\n  ```bash\n  wandb launch --help\n  ```\n  "}
{"question":"Can Launch automatically provision (and spin down) compute resources for me in the target environment?","answer":"Yes. This depends on the environment, we are able to provision resources in SageMaker, and Vertex. In Kubernetes, autoscalers can be used to automatically spin up and spin down resources when required. The Solution Architects at W&B are happy to work with you to configure your underlying Kubernetes infrastructure to facilitate retries, autoscaling, and use of spot instance node pools. Reach out to support@wandb.com or your shared Slack channel."}
{"question":"Is `wandb launch -d` or `wandb job create image` uploading a whole docker artifact and not pulling from a registry?","answer":"No. The  `wandb launch -d` command will not upload to a registry for you. You need to upload your image to a registry yourself. Here are the general steps:\n\n1. Build an image. \n2. Push the image to a registry.\n\nThe workflow looks like:\n\n```bash\ndocker build -t <repo-url>:<tag> .\ndocker push <repo-url>:<tag>\nwandb launch -d <repo-url>:<tag>\n```\n\nFrom there, the launch agent will spin up a job pointing to that container.  Here is an example of how to give the agent access to pull an image from a container registry in AWS:\n\nWithin your launch agent config (launch-config.yaml), provide the name of the target resource environment and the container registry for the environment and registry keys, respectively.\n\n```\nenvironment:\n  type: aws\n  region: <aws-region>\nregistry:\n  type: ecr\n  # URI of the ECR repository where the agent will store images.\n  # Make sure the region matches what you have configured in your\n  # environment.\n  uri: <account-id>.ecr.<aws-region>.amazonaws.com\/<repository-name>\n  # Alternatively, you can simply set the repository name\n  # repository: my-repository-name\n```\n\nFor Kubernetes, the Kubernetes cluster pods will need access to the registry you are pushing to. \n"}
{"question":"Can I specify a Dockerfile and let W&B build a Docker image for me?","answer":"Yes. This is particularly useful if you have a lot of requirements that do not change often, but you have a codebase that does change often.\n\n:::important\nEnsure your Dockerfile is formatted to use mounts. \n:::\n\nOnce your Dockerfile is configured, you can then specify your Dockerfile in one of three ways to W&B:\n\n* Use Dockerfile.wandb\n* W&B CLI\n* W&B App\n\n**DockerFile**\n\nInclude a file called `Dockerfile.wandb` in the  same directory as the W&B run\u2019s entrypoint.  W&B will use `Dockerfile.wandb` instead of W&B\u2019s built-in Dockerfile.\n\n**CLI**\n\nProvide the `--dockerfile` flag when you call queue a launch job with the `wandb launch`\n\n```\nwandb launch --dockerfile path\/to\/Dockerfile\n```\n\n\n**App**\n\nWhen you add a job to a queue on the W&B App, provide the path to your Dockerfile in the **Overrides** section. More specifically, provide it as a key-value pair where `\"dockerfile\"` is the key and the value is the path to your Dockerfile. \n\nFor example, the following JSON shows how to include a Dockerfile that is within a local directory:\n\n```\n{\n  \"args\": [],\n  \"run_config\": {\n    \"lr\": 0,\n    \"batch_size\": 0,\n    \"epochs\": 0\n  },\n  \"entrypoint\": [],\n  \"dockerfile\": \".\/Dockerfile\"\n}\n```\n"}
{"question":"How do I control who can push to a queue?","answer":"Queues are scoped to a team of users. You define the owning entity when you create the queue.  To restrict access, you can change the team membership."}
{"question":"What permissions does the agent require in Kubernetes?","answer":"The following kubernetes manifest will create a role named `wandb-launch-agent` in the`wandb`namespace. This role will allow the agent to create pods, configmaps, secrets, and pods\/log in the `wandb` namespace. The `wandb-cluster-role` will allow the agent to create pods, pods\/log, secrets, jobs, and jobs\/status in any namespace of your choice."}
{"question":"Does Launch support parallelization?  How can I limit the resources consumed by a job?","answer":"Yes, Launch supports scaling jobs across mulitple GPUs and multiple nodes.\n\nOn an inter-job level, an individual launch agent is configured with a `max_jobs` parameter that determines how many jobs that agent can run simultaneously. Additionally, you can point to as many agents as you want at a particular queue, so long as those agents are connected to an infrastructure that they can launch into.\n  \nYou can limit the CPU\/GPU, memory, and other requirements at the launch queue or job run level, in the resource config. For more information about setting up queues with resource limits on Kubernetes. \n\n   \nFor sweeps, in the SDK you can add a block to the queue config\n\n```\n  scheduler:\n    num_workers: 4\n```\n\nTo limit the number of concurrent runs from a sweep that will be run in parallel.\n\n"}
{"question":"When using Docker queues to run multiple jobs that download the same artifact with `use_artifact`, do we re-download the artifact for every single run of the job, or is there any caching going on under the hood?","answer":"There is no caching; each job is independent.  However, there are ways to configure your queue\/agent where it mounts a shared cache.  You can achieve this via docker args in the queue config.\n\nAs a special case, you can also mount the W&B artifacts cache as a persistent volume.\n"}
{"question":"Can you specify secrets for jobs\/automations? For instance, an API key which you do not wish to be directly visible to users?","answer":"Yes. The suggested way is:\n\n  1. Add the secret as a vanilla k8s secret in the namespace where the runs will be created. something like `kubectl create secret -n <namespace> generic <secret_name> <secret value>`\n\n 2. Once that secret is created, you can specify a queue config to inject the secret when runs start. The end users cannot see the secret, only cluster admins can.\n"}
{"question":"How can admins restrict what ML engineers have access to modify? For example, changing an image tag may be fine but other job settings may not be.","answer":"This can be controlled by queue config templates, which expose certain queuefields for non-team-admin users to edit within limits defined by admin users. Only team admins can create or edit queues, including defining which fields are exposed and the limits for them."}
{"question":"How does W&B Launch build images?","answer":"In `Launch`, the process of building images is tailored according to the source of the job and whether an accelerator base image is specified in the resource configuration. Here's an explanation of the steps taken during the image build process:\n\n**General Setup**\n\nWhen specifying a queue config or submitting a job, you can provide a base accelerator image in the queue or job resource configuration:\n```\n{\n    \"builder\": {\n        \"accelerator\": {\n            \"base_image\": \"image-name\"\n        }\n    }\n}\n```\n\n**Image Build Steps**\n\n- **Jobs Sourced from Git or Code (without a specified accelerator image):**\n  - Python packages are installed as required by the job.\n  - A user and working directory are created within the image.\n  - The source code is copied into the image.\n  - The image is configured with an appropriate entrypoint to initiate the job.\n\n- **Jobs Sourced from Git or Code (with a specified accelerator image):**\n  - Python is installed using the apt package manager to match the accelerator's requirements.\n  - Python packages are installed as needed.\n  - A user and working directory are set up.\n  - The source code is incorporated into the image.\n  - An entrypoint is established to run the job.\n\n- **Jobs Sourced from an Existing Image:**\n  - No additional steps are taken; the pre-existing image is used as is.\n\nThis setup ensures that the build process is optimized for the job's requirements and the computational resources specified, facilitating efficient and effective job execution.\n"}
{"question":"What requirements does the accelerator base image have?","answer":"For jobs that use an accelerator, an accelerator base image with the required accelerator components installed can be provided. Other requirements for the provided accelerator image include:\n\n- Debian compatibility (the Launch Dockerfile uses apt-get to fetch python )\n- Compatibility CPU & GPU hardware instruction set (Make sure your CUDA version is supported by the GPU you intend on using)\n- Compatibility between the accelerator version you provide and the packages installed in your ML algorithm\n- Packages installed that require extra steps for setting up compatibility with hardware\n"}
{"question":"How do I make W&B Launch work with Tensorflow on GPU?","answer":"For jobs that use tensorflow on GPU, you may also need to specify a custom base image for the container build that the agent will perform in order for your runs to properly utilize GPUs. This can be done by adding an image tag under the `builder.accelerator.base_image` key to the resource configuration. For example:\n\n```\n{\n    \"gpus\": \"all\",\n    \"builder\": {\n        \"accelerator\": {\n            \"base_image\": \"tensorflow\/tensorflow:latest-gpu\"\n        }\n    }\n}\n```\n\n**Note prior to wandb version: 0.15.6 use `cuda` instead of `accelerator` as the parent key to `base_image`.**\n"}
{"question":"Can you use a custom repository for packages when Launch builds the image?","answer":"Yes. To do so, add the following line to your `requirements.txt` and replace the values passed to `index-url` and `extra-index-url` with your own values:\n\n```\n----index-url=https:\/\/xyz@<your-repo-host> --extra-index-url=https:\/\/pypi.org\/simple\n```\n \nThe `requirements.txt` needs to be defined at the base root of the job.\n"}
{"question":"How can I Automatically run re-queuing on preemption in `Launch`","answer":"In some cases, it can be useful to set up jobs to be resumed after they are interrupted.  For example, you might run broad hyperparameter sweeps on spot instances, and want them to pick up again when more spot instances spin up.  Launch can support this configuration on Kubernetes clusters.\n\nIf your Kubernetes queue is running a job on a node that\u2019s pre-empted by a scheduler, the job will be automatically added back to the end of the queue so it can resume later. This resumed run will have the same name as the original, and can be followed from the same page in the UI as the original. A job can be automatically re-queued this way up to five times. \n\nLaunch detects whether a pod is preempted by a scheduler by checking if the pod has the condition `DisruptionTarget` with one of the following reasons:\n\n- `EvictionByEvictionAPI`\n- `PreemptionByScheduler`\n- `TerminationByKubelet`\n\nIf your job\u2019s code is structured to allow resuming, it will enable these re-queued runs to pick up where they left off. Otherwise, runs will start from the beginning when they are re-queued.  \n\nThere is currently no way to opt out of automatic run re-queuing for preempted nodes. However, if you delete a run from the UI or delete the node directly, it will not be re-queued.\n\nAutomatic run re-queuing is currently only available on Kubernetes queues; Sagemaker and Vertex are not yet supported.\"\n"}
{"question":"Do I need to provide values for all hyperparameters as part of the W&B Sweep. Can I set defaults?","answer":"The hyperparameter names and values specified as part of the sweep configuration are accessible in `wandb.config`, a dictionary-like object.\n\nFor runs that are not part of a sweep, the values of `wandb.config` are usually set by providing a dictionary to the `config` argument of `wandb.init`. During a sweep, however, any configuration information passed to `wandb.init` is instead treated as a default value, which might be over-ridden by the sweep.\n\nYou can also be more explicit about the intended behavior by using `config.setdefaults`. The following code snippets for both methods are shown below:\n\nusing wandb.init\n\n```\n# set default values for hyperparameters\nconfig_defaults = {\"lr\": 0.1, \"batch_size\": 256}\n\n# start a run, providing defaults\n#   that can be over-ridden by the sweep\nwith wandb.init(config=config_default) as run:\n    # add your training code here\n    ...\n```\n\nusing config.setdefaults\n\n```\n# set default values for hyperparameters\nconfig_defaults = {\"lr\": 0.1, \"batch_size\": 256}\n\n# start a run\nwith wandb.init() as run:\n    # update any values not set by sweep\n    run.config.setdefaults(config_defaults)\n\n    # add your training code here\n```  \n"}
{"question":"How should I run sweeps on SLURM?","answer":"When using sweeps with the SLURM scheduling system, we recommend running `wandb agent --count 1 SWEEP_ID` in each of your scheduled jobs, which will run a single training job and then exit. This makes it easier to predict runtimes when requesting resources and takes advantage of the parallelism of hyperparameter search."}
{"question":"Can I rerun a grid search?","answer":"Yes. If you exhaust a grid search but want to re-execute some of the W&B Runs (for example because some crashed). Delete the W&B Runs - ones you want to re-execute, then choose the **Resume** button on the sweep control page. Finally, start new W&B Sweep agents with the new Sweep ID.\n\n**Note: Parameter combinations with completed W&B Runs are not re-executed.**\n"}
{"question":"How do I use custom CLI commands with sweeps?","answer":"You can use W&B Sweeps with custom CLI commands if you normally configure some aspects of training by passing command line arguments.\n\nFor example, the following code snippet demonstrates a bash terminal where the user is training a Python script named train.py. The user passes in values that are then parsed within the Python script:\n\n```\npython train.py -b     your-training-config     --batchsize 8     --lr 0.00001\n```\n\nTo use custom commands, edit the `command` key in your YAML file. For example, continuing the example above, that might look like so:\n\n```\nprogram:\n  train.py\nmethod: grid\nparameters:\n  batch_size:\n    value: 8\n  lr:\n    value: 0.0001\ncommand:\n  - ${env}\n  - python\n  - ${program}\n  - \"-b\"\n  - your-training-config\n  - ${args}\n```\n\nThe `${args}` key expands to all the parameters in the sweep configuration file, expanded so they can be parsed by `argparse: --param1 value1 --param2 value2`\n\nIf you have extra arguments that you don't want to specify with `argparse` you can use:\n\n```\nparser = argparse.ArgumentParser()\nargs, unknown = parser.parse_known_args()\n```\n\n:::info\nDepending on the environment, `python` might point to Python 2. To ensure Python 3 is invoked, use `python3` instead of `python` when configuring the command:\n\n```\nprogram:\n  script.py\ncommand:\n  - ${env}\n  - python3\n  - ${program}\n  - ${args}\n```\n:::\n\n"}
{"question":"Is there a way to add extra values to a sweep, or do I need to start a new one?","answer":"You cannot change the Sweep configuration once a W&B Sweep has started. But you can go to any table view, and use the checkboxes to select runs, then use the **Create sweep** menu option to create a new Sweep configuration using prior runs."}
{"question":"Can we flag boolean variables as hyperparameters?","answer":"You can use the `${args_no_boolean_flags}` macro in the command section of the config to pass hyperparameters as boolean flags. This will automatically pass in any boolean parameters as flags. When `param` is `True` the command will receive `--param`, when `param` is `False` the flag will be omitted."}
{"question":"Can I use Sweeps and SageMaker?","answer":"Yes. At a glance, you will need to need to authenticate W&B and you will need to create a `requirements.txt` file if you use a built-in SageMaker estimator. For more on how to authenticate and set up a requirements.txt file.\n\nBelow is a simplified example demonstrating how to configure a SageMaker training job using PyTorch and W&B Sweeps on the CIFAR-10 dataset. The example highlights defining the SageMaker estimator, setting up the sweep configuration, and launching the hyperparameter tuning job.\n\n**Define the SageMaker Estimator**\n\nHere, you define the instance and global hyperparameters for the SageMaker estimator:\n\n```\nestimator = PyTorch(entry_point=\"cifar10.py\",\n                    source_dir=os.getcwd() + \"\/source\",\n                    role=role,\n                    framework_version='1.0.0.dev',\n                    train_instance_count=1,\n                    train_instance_type='ml.p2.xlarge',\n                    hyperparameters={\n                        'epochs': 50,\n                        'momentum': 0.9\n                    })\n```\n\n**Define the W&B Sweep Configuration**\n\nThis part includes the definition of the hyperparameter ranges and the metric to be optimized:\n\n```\nfrom sagemaker.tuner import ContinuousParameter, IntegerParameter, CategoricalParameter, HyperparameterTuner\n\nhyperparameter_ranges = {\n    'lr': ContinuousParameter(0.0001, 0.01),\n    'hidden_nodes': IntegerParameter(20, 100),\n    'batch_size': CategoricalParameter([128, 256, 512]),\n    'conv1_channels': CategoricalParameter([32, 64, 128]),\n    'conv2_channels': CategoricalParameter([64, 128, 256, 512]),\n}\n\nobjective_metric_name = 'average test accuracy'\nobjective_type = 'Maximize'\nmetric_definitions = [{'Name': 'average test accuracy',\n                       'Regex': 'Test Accuracy: ([0-9\\.]+)'}]\n\ntuner = HyperparameterTuner(estimator,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=1,\n                            max_parallel_jobs=1,\n                            objective_type=objective_type)\n```\n\n**Launch the Sweep**\n\nFinally, you start the hyperparameter tuning job by fitting the tuner with the training data inputs:\n```\ninputs = {'training': s3_input_data}  # Adjust this according to your S3 data path\ntuner.fit({'training': inputs})\n```\n\n"}
{"question":"Can you use W&B Sweeps with cloud infrastructures such as AWS Batch, ECS, etc.?","answer":"In general, you would need a way to publish `sweep_id` to a location that any potential W&B Sweep agent can read and a way for these Sweep agents to consume this `sweep_id` and start running.\n\nIn other words, you need something that can invoke `wandb agent`. For instance, bring up an EC2 instance and then call `wandb agent` on it. In this case, you might use an SQS queue to broadcast `sweep_id` to a few EC2 instances and then have them consume the `sweep_id` from the queue and start running.\n"}
{"question":"How can I change the directory my sweep logs to locally?","answer":"You can change the path of the directory where W&B will log your run data by setting an environment variable `WANDB_DIR`. For example:\n\n```\nos.environ[\"WANDB_DIR\"] = os.path.abspath(\"your\/directory\")\n```\n"}
{"question":"How can I use sweeps to optimize for multiple metrics","answer":"If you want to optimize multiple metrics in the same run, you can use a weighted sum of the individual metrics.\n\n```\nmetric_combined = 0.3 * metric_a + 0.2 * metric_b + ... + 1.5 * metric_n\nwandb.log({\"metric_combined\": metric_combined})\n```\n\nEnsure to log your new combined metric and set it as the optimization objective:\n\n```\nmetric:\n  name: metric_combined\n  goal: maximize\n```\n"}
{"question":"How do I enable code logging with Sweeps?","answer":"To enable code logging for sweeps, simply add `wandb.log_code()` after you have initialized your W&B Run. This is necessary even when you have enabled code logging in the settings page of your W&B profile in the app."}
{"question":"What is the \"Est. Runs\" column?","answer":"The \"Est. Runs\" column in the W&B Sweep UI displays an estimate of the total number of runs expected for a given Sweep. This estimate is calculated based on the size of the search space defined in your Sweep configuration. Specifically, it represents the cartesian product of the values across all hyperparameters.\n\nFor instance, if your Sweep configuration specifies two parameters with three discrete values each, as in the example you provided with the parameters:\n\n- **param1**: values [1, 2, 3]\n- **param2**: values [1, 2, 3]\n\nThe cartesian product of these values would be \\(3 \times 3 = 9\\). Therefore, the \"Est. Runs\" will display the number \"9\", indicating that there are nine possible combinations of these parameters, and thus nine potential runs.\n\nYou can also programmatically obtain this estimated number of runs using the W&B SDK. After creating a Sweep, you can retrieve the Sweep object via the API and access its `expected_run_count` attribute to get the estimated number of runs. Here's how you might do it in code:\n\n```\nimport wandb\nsweep_id = wandb.sweep(sweep_configs, project=\"your_project_name\", entity=\"your_entity_name\")\napi = wandb.Api()\nsweep = api.sweep(f\"your_entity_name\/your_project_name\/sweeps\/{sweep_id}\")\nprint(f\"EXPECTED RUN COUNT = {sweep.expected_run_count}\")\n```\n\nThis approach allows you to see and verify the expected number of runs based on the search space you've defined in your Sweep configuration, directly in the W&B interface or programmatically through your code.\n"}
{"question":"Who has access to my artifacts?","answer":"Artifacts inherit the access to their parent project:\n\n- If the project is private, then only members of the project's team have access to its artifacts.\n- For public projects, all users have read access to artifacts but only members of the project's team can create or modify them.\n- For open projects, all users have read and write access to artifacts.\n"}
{"question":"How do I log an artifact to an existing run?","answer":"Occasionally, you may want to mark an artifact as the output of a previously logged run. In that scenario, you can reinitialize the old run and log new artifacts to it as follows:\n\n```\nwith wandb.init(id=\"existing_run_id\", resume=\"allow\") as run:\n    artifact = wandb.Artifact(\"artifact_name\", \"artifact_type\")\n    artifact.add_file(\"my_data\/file.txt\")\n    run.log_artifact(artifact)\n```\n"}
{"question":"How do I set a retention or expiration policy on my artifact?","answer":"If you have artifacts that are subject to data privacy regulations such as dataset artifacts containing PII, or want to schedule the deletion of an artifact version to manage your storage, you can set a TTL (time-to-live) policy."}
{"question":"How can I find the artifacts logged or consumed by a run? How can I find the runs that produced or consumed an artifact?","answer":"W&B automatically tracks the artifacts a given run has logged as well as the artifacts a given run has used and uses the information to construct an artifact graph -- a bipartite, directed, acyclic graph whose nodes are runs and artifacts.\n\nYou can walk this graph programmatically with the Public API, starting from either a run or an artifact.\n\nfrom an artifact,\n\n```\napi = wandb.Api()\n\nartifact = api.artifact(\"project\/artifact:alias\")\n\n# Walk up the graph from an artifact:\nproducer_run = artifact.logged_by()\n# Walk down the graph from an artifact:\nconsumer_runs = artifact.used_by()\n\n# Walk down the graph from a run:\nnext_artifacts = consumer_runs[0].logged_artifacts()\n# Walk up the graph from a run:\nprevious_artifacts = producer_run.used_artifacts()\n```\n\nfrom a run,\n\n```\napi = wandb.Api()\n\nrun = api.run(\"entity\/project\/run_id\")\n\n# Walk down the graph from a run:\nproduced_artifacts = run.logged_artifacts()\n# Walk up the graph from a run:\nconsumed_artifacts = run.used_artifacts()\n\n# Walk up the graph from an artifact:\nearlier_run = consumed_artifacts[0].logged_by()\n# Walk down the graph from an artifact:\nconsumer_runs = produced_artifacts[0].used_by()\n```\n"}
{"question":"How do I best log models from runs in a sweep?","answer":"One effective pattern for logging models in a sweep is to have a model artifact for the sweep, where the versions will correspond to different runs from the sweep. More concretely, you would have:\n\n```\nwandb.Artifact(name=\"sweep_name\", type=\"model\")\n```\n"}
{"question":"How do I find an artifact from the best run in a sweep?","answer":"You can use the following code to retrieve the artifacts associated with the best performing run in a sweep:\n\n```\napi = wandb.Api()\nsweep = api.sweep(\"entity\/project\/sweep_id\")\nruns = sorted(sweep.runs, key=lambda run: run.summary.get(\"val_acc\", 0), reverse=True)\nbest_run = runs[0]\nfor artifact in best_run.logged_artifacts():\n    artifact_path = artifact.download()\n    print(artifact_path)\n```\n"}
{"question":" How do I save code in an artifact?\u200c","answer":"Use `save_code=True` in `wandb.init` to save the main script or notebook where you\u2019re launching the run. To save all your code to a run, version code with Artifacts. Here\u2019s an example:\n\n```\ncode_artifact = wandb.Artifact(type=\"code\")\ncode_artifact.add_file(\".\/train.py\")\nwandb.log_artifact(code_artifact)\n```\n"}
{"question":"Using artifacts with multiple architectures and runs?","answer":"There are many ways in which you can think of _version_ a model. Artifacts provides a you a tool to implement model versioning as you see fit. One common pattern for projects that explore multiple model architectures over a number of runs is to separate artifacts by architecture. As an example, one could do the following:\n\n1. Create a new artifact for each different model architecture. You can use `metadata` attribute of artifacts to describe the architecture in more detail (similar to how you would use `config` for a run).\n2. For each model, periodically log checkpoints with `log_artifact`. W&B will automatically build a history of those checkpoints, annotating the most recent checkpoint with the `latest` alias so you can refer to the latest checkpoint for any given model architecture using `architecture-name:latest`\n"}
{"question":"How can I fetch these Version IDs and ETags in W&B?","answer":"If you've logged an artifact reference with W&B and if the versioning is enabled on your buckets then the version IDs can be seen in the S3 UI. To fetch these version IDs and ETags in W&B, you can fetch the artifact and then get the corresponding manifest entries. For example:\n\n```\nartifact = run.use_artifact(\"my_table:latest\")\nfor entry in artifact.manifest.entries.values():\n    versionID = entry.extra.get(\"versionID\")\n    etag = manifest_entry.extra.get(\"etag\")\n```\n"}
{"question":"What is the artifact of type job I see in my artifacts page ?","answer":"A launch job is a specific type of W&B Artifact that represents a task to complete. For example, common launch jobs include training a model or triggering a model evaluation. Job definitions include:\n\n- Python code and other file assets, including at least one runnable entrypoint.\n- Information about the input (config parameter) and output (metrics logged).\n- Information about the environment. (for example, `requirements.txt`, base `Dockerfile`).\n\nCertainly! Here's the information from the table about job types, restructured into a bullet-point format for clarity:\n\nThere are three main kinds of job definitions:\n\n- **Artifact-based (or code-based) jobs**:\n  - **Definition**: The code and other assets are saved as a W&B artifact.\n  - **How to run this job type**: These jobs require a Launch agent that is configured with a builder to run.\n\n- **Git-based jobs**:\n  - **Definition**: The code and other assets are cloned from a specific commit, branch, or tag in a git repository.\n  - **How to run this job type**: Running these jobs requires a Launch agent configured with a builder and git repository credentials.\n\n- **Image-based jobs**:\n  - **Definition**: The code and other assets are incorporated into a Docker image.\n  - **How to run this job type**: These jobs may require a Launch agent configured with image repository credentials to run.\n\n**Additional Tips**\nWhile Launch jobs can serve various purposes, including deploying models to a Triton inference server, all jobs must invoke `wandb.init` to ensure successful completion. This invocation creates a run that is tracked in the W&B workspace.\n\nTo manage and execute jobs, navigate to the `Jobs` tab of your project workspace in the W&B App. From there, jobs can be configured and dispatched to a launch queue to be executed on specified target resources.\n"}